\chapter{Discussion}
\label{ch:Discussion}
\section{Overview}
In this dissertation, I set out to evaluate if a univariate conditional volatility forecast could be improved by adding an exogenous covariate to the already existing model. First, data was extracted from Yahoo Finance's database and transformed into continuous returns which were then modelled empirically using grid-search selected $\mathbf{EGARCH}$ models. Then, after receiving Irithmic's data provided, these data were transformed via the weight decaying algorithm. Once the data processing was complete, I trained the optimal models and forecasted and compared outputs. Following the univariate modelling, I implemented a Dynamic Conditional Correlation model to evaluate the relationship between the selected exogenous covariate and the modelled conditional volatility. Though yielding mediocre results, I continued with the final process of fitting an $\mathbf{EGARCH}$ model with an exogenous covariate. Disappointingly, the results of the model were not improved, though a greater understanding of conditional volatility modelling, exogenous data model integration, and many ideas for future work were derived. This project approached a well researched area of volatility modelling, with a novel approach of taking a tangentially related piece of non-standard data, standardizing the format in a generalisible fashion and implemented it using empirical methods.  


\section{Future Work}
There is great opportunity for further work on this topic. First, I think there is great potential for further incorporating exogenous variables to conditional volatility forecasts. There is research in the area, but has plenty of room for continued work. I think incorporating linear and non-linear models as an extension to $\mathbf{EGARCH}$ models could be very powerful. Of course there is the balance in statistics between interpretation and prediction, but if one is trying to simply improve forecast accuracy, finding exogenous data sets (could be very large data) to accompany the empirical model could be very powerful. I also think there is opportunity for great model improvements using the Dynamic Conditional Correlation models in a similar study. Given the constraints, I was only able to implement it once, but the possibility for incorporating more data such as interest rates, the FTSE 100 index, or an aggregate of the sector the targeted stock is in. Second, I believe there is much greater opportunity to explore avenues to more effectively use the Irithmics data. Though it was not proved in this project, the power of the Wisdom of the Crowds cannot be understated. Having information about the predicted behavior of trillion's of pounds of assets moving in Britain has many use cases beyond just volatility predictions. I believe a possible greater aggregation of the data across the entire FTSE 100, rather than a selected group of stocks could yield better results, as the greater grouping could smooth out anomalies and increase predictability. Lastly, there is great room to contribute to the Python and R time series open source community. As the research transcends traditional models, there is great opportunity to contribute to development of time series modelling libraries of greater complexity (rugarch in R, arch in Python). I have greatly enjoyed the artistry of programming and developing models and am greatly excited by the opportunity to contribute further to the literature in Financial Time Series. 

\section{Limitations}
As this research completed for an M.Sc. Dissertation, there were limitations to completing a fruitful study. Firstly was the datset size. As Irithmics graciously donated these data free of charge, to explore a novel concept, I did not have 1000's trading day history for each stock, instead about 252. This lead to challenges in fitting and training a model with holdout data for out of sample testing, to identify generalization error and tune hyperparameters, while balancing the amount of data for a large sample to train with, though this can also be attributed to the challenges of working with time series data. For example, traditional methods for training neural networks or tree models like bootstrapping or k-fold cross validation are not available as testing data outside the temporal constraint is not useful. Second, the constraint of time. Given this project was completed in 2-3 months, I had to make decisions and pursue ideas very quickly and there was no time to restart or take a step back, read full textbooks and many papers. Lastly, the constraint of the exogenous data itself. The Irithmics data is the corporation's proprietary models prediction of what the aggregate decision of the funds to short/sell a stock. The maximum probability was about 0.07, so even if 7\% of the 250,000 funds decided they would sell the stock, depending on the individual portfolio weights, and actions of other institutions, macroeconomic events and other investors, there was no reason to say this will increase the instantaneous volatility of the stock on that day.  


